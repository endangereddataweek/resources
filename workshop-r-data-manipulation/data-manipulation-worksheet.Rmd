---
title: "Data Manipulation"
output: html_document
---

Load some necessary packages.

```{r, include=FALSE}
library(dplyr)
library(tidyr)
```

And let's read some data that we'll be working with.

```{r}
midwest_census <- read.csv("midwest-census.csv")
```

# Select

We can select which columns we want by putting the name of the column in the `select()` function. Here we pick three columns.

```{r}
midwest_census %>% 
  select(year, state, totalPopulation)
```

We can also get rid of columns by using the `-` sign. The `starts_with()` and `ends_with()` functions are useful.

Read the documentation for this function, `?select`.

Select the `year` and `state` columns, and all the columns that end with the word `Population`.

```{r}
{{DELETE}}
midwest_census %>%
  select(year, state, ends_with("Population"))
```

Remove the column `GISJOIN`.

```{r}
{{DELETE}}
midwest_census %>%
  select(-GISJOIN)
```

Pick just the columns that you want:

```{r}

```

# Filter

Filtering is more interesting. To keep certain rows, we have to pass the `filter()` function a vector of `TRUE` and `FALSE` values, one for each row. The most common way to do that is to use a comparison operator on a column of the data.

```{r}
midwest_census %>% 
  filter(state == "Nebraska")
```

```{r}
midwest_census %>% 
  filter(year > 1890) 
```

Can you get just the counties with a total population more than 10,000 people?

```{r}
{{DELETE}}
midwest_census %>%
  filter(totalPopulation > 10000)
```

Can you get just the counties between 1820 and 1870?

```{r}
{{DELETE}}
midwest_census %>%
  filter(year > 1820 & year < 1870)
```

Get just the rows from Kansas in 1890.

```{r}
{{DELETE}}
midwest_census %>%
  filter(year == 1890, state == "Kansas")
```

# Arrange

The `arrange()` function lets us sort. Often we want to sort a data frame by one of its columns. This can be done with the verb `arrange()`. By default `arrange()` will sort from least to greatest; we can use the function `desc()` to sort from greatest to least. In this example, we sort the data frame to get the cities with the highest number of people. Here are the biggest counties in 1830. (Notice that we can create a pipeline of functions.)

```{r}
midwest_census %>% 
  filter(year == 1830) %>% 
  arrange(desc(totalPopulation))
```

Which counties had the highest number of African Americans?

```{r}
{{DELETE}}
midwest_census %>%
  select(totalAfAmPopulation, county, state, year) %>%
  arrange(desc(totalAfAmPopulation))
```

# Mutate

The `mutate()` function lets us create new columns out of existing columns. Perhaps you would like a column that calculates the percentage of African Americans by county. We could do that:

```{r}
midwest_census %>%
  mutate(percentageAfAm = round(100 * totalAfAmPopulation / totalPopulation))
```

Can you calculate the percentage of the population that is white? The percentage of the population that is black? Can you create a `population_nonwhite` column?

```{r}

```

# Group by and summarize

Notice that in the example above the `arrange()` function sorted the entire data frame. So when we looked for the counties with the largest number of people, we got rows from 2000, then 1990, then 1980, then 1970, and so on. What if we wanted to get the biggest county from each year?

We can solve this kind of problem with what Hadley Wickham calls the "split-apply-combine" pattern of data analysis. Think of it this way. First we can *split* the big data frame into separate data frames, one for each year. Then we can *apply* our logic to get the results we want; in this case, that means sorting the data frame. We might also want to get just the top one row with the biggest number of people. Then we can *combine* those split apart data frames into a new data frame.

Observe how this works. If we want to get the county with the most black people in each year, we can use the following code:

```{r}
midwest_census %>% 
  select(year, totalAfAmPopulation, county, state) %>% 
  group_by(year) %>% 
  arrange(desc(totalAfAmPopulation)) %>% 
  slice(5) 
```

Let's walk through that logic step by step. 

1. First, we select only the columns that we are interested in, namely, the column for the year, the total black population, the county, and the year. We do this just so that the results print out in a useful way: in a real analysis we might decide not to throw away the other columns.
2. The crucial step is when we `group_by()` the `year`. This creates a new data-frame (the *split* step) for each unique combination of values in the variables. (Note that you can group by combinations of columns, so, one group for each combination of city and state, for instance.)
4. Next we *apply* our logic, in this case, sorting by the column `totalAfAmPopulation` in descending order. This puts the rows with the biggest value at the top.
5. Next we continue to *apply* our logic with `slice()`. This function simply gives us the rows in each of the split-up data frames with that index. So `slice(1)` gives us the first row, `slice(5)` gives us this fifth row, and `slice(1:5)` gives us the first through fifth rows. 
6. The last step, *combine*, where the split-up data frames are brought back together, is done for us automatically. Note that the data frame is still grouped, however, so any subsequent data manipulation verbs will be applied to the groups rather than the whole data frame. If we wished, we could use `ungroup()`.

This particular operation, getting the top value in a split up data frame is so common that dplyr provides us with a `top_n()` function as a short cut. That function also handles ties better. (What if, for instance, two counties both have the same biggest value?)

```{r}
midwest_census %>% 
  select(year, totalAfAmPopulation, county, state) %>% 
  group_by(year) %>% 
  top_n(1, totalAfAmPopulation)
```

We get the same results more concisely and reliably, though the steps of "split-apply-combine" are perhaps somewhat less easy to see.

The data that we are currently working with has one row for each combination of a **year** and a **population**. We might want to know the total number of African Americans for each year. To do this, we need to group by a variable, then use an aggregation function.

```{r}
midwest_census %>% 
  group_by(year) %>% 
  summarize(counties = n(),
            totalAfAm = sum(totalAfAmPopulation))
```

For each year, which was the biggest county?

```{r}

```

For the year 1910, what was the largest county in each state?

```{r}

```

For each year, what were the three largest counties in Nebraska?

```{r}

```

## Summarizing or aggregating data (`summarize()`)

In the examples using `top_n()` or `slice()` we performed a very simple kind of data summary, where we took the single row with the biggest value in a given column. This essentially boiled many rows of a data frame down into a single row. We would like to be able to summarize or aggregate a data frame in other ways as well. For instance, we often want to take the sum or the mean of a given column. We can do this using the `summarize()` function in conjunction with the `group_by()` function.

In this example, we group by the year. Then we find the total number of slaves for each year.

```{r}
midwest_census %>% 
  group_by(year) %>% 
  summarize(total_slaves = sum(slavePopulation, na.rm = TRUE))
```

Notice that we get one row in the recombined data frame for each group in the original data frame. The value in the new column is the result of a function (in this case, `sum()`) applied to the columns in each of the split apart data frames.

There is also a special case where we might want to know how many rows were in each of the split apart (or grouped) data frames. We can use the special `n()` function to get that count. (Just like the case of `slice()` and `top_n()`, this is such a common thing to do that dplyr provides the special functions `count()` and `tally()`. You can look up their documentation to see how they work.)

```{r}
midwest_census %>% 
  group_by(county) %>% 
  summarize(total_counties = n())
```

What is the average number of white and average number of black people the total population for each year since 1840?

```{r}

```

What was the average number of people in each county for each year?

```{r}

```

What was the average percentage of black people in each county for each year?

```{r}

```

## Data reshaping (`spread()` and `gather()`)

It can be helpful to think of tabular data as coming in two forms: wide data, and long data. Let's load two sets of sample data to get a sense of how these work.

```{r}
population <- read.csv("population.csv")
population

cases <- read.csv("cases.csv")
cases
```

The first thing we can notice about this data frame is that it is very wide because it has a column for each of the countries. The data is also suitable for reading because it reads like a table in a publication. We can read from left to right and see population or case values for each year. The difficulties of computing on or plotting the data will also become quickly apparent. How would you make a plot of the change over time? Or how would you filter by year, or summarize by year? For that matter, what do the numbers in the table represent, since they are not given an explicit variable name?

The problem with the table is that it is not *tidy data*, because the variables are not in columns and observations in rows. One of the variables is the year, but its values are in the column headers. And another of the variables is population, but its values are spread across rows and columns and it is not explicitly named. 

The `gather()` function from the [tidyr](https://cran.rstudio.com/web/packages/tidyr/) package lets us turn wide data into long data. We need to tell the function two kinds of information. First we need to tell it the name of the column to create from the column headers and the name of the implicit variable in the rows. In the example below, we create two new columns `year` and `cases`. Then we also have to tell the function if there are any columns which should remain unchanged. If that were the case, we would remove it from the gathering using the same syntax as the `select()` function.

```{r}
population %>% 
  # we want the last two columns, so we tell R to select the range of columns from 2-3
  gather(year, population, 2:3) 
```

We can see the results above.

The inverse operation of `gather()` is `spread()`. With `spread()` we specify the name of the column which should become the new column headers (in this case `cases` and `population`), and then the name of the column to fill in underneath those new column headers (in this case, `key` and `value`). We can see the results below.

```{r}
cases %>% 
  spread(key, value)
```

Just by looking at the data we can see that we got back to where we started, but we can also verify that programmatically using the `identical()` function.

Turning long data into wide is often useful when you want to create a tabular representation of data. (And once you have a data frame that can be a table, the `knitr::kable()` function is quite nice.) And some algorithms, such as clustering algorithms, expect wide data rather than tidy data.

For the exercise, we will use summary statistics of the number of white and black members in the Methodists by year.

```{r}
states_by_year_race <- midwest_census %>% 
  group_by(year) %>% 
  summarize(white = sum(totalWhitePopulation, na.rm = TRUE),
            black = sum(totalAfAmPopulation, na.rm = TRUE),
            indian = sum(totalIndianPopulation, na.rm = TRUE))
states_by_year_race
```

The data in `states_by_year_race` could be tidier still. While `white`, `black`, and `indian` are variables, it is perhaps better to think of them as two different variables. One variable would be `race`, containing the racial descriptions that the Census used, and another would be `population`, containing the number of members. Using the `gather()` function, create that data frame.

```{r}

```

Now use that newly tidied data frame to create a wide data frame, where the years are the column headers and the racial descriptions are the rows.

```{r}

```

Now use the same tidied data to create a wide data frame where the racial descriptions are column headers and the years are rows.

```{r}

```

# Capstone

Can you aggregate the data by year and state to compute the change percentages of African American population? Can you make a nice table of the data using the `knitr::kable()` function? 

```{r}

```
